---
title: "Predicting Healthcare Expenditures for Individuals with Breast Cancer: Considering Chronic Comorbidities within a Two-Stage Machine Learning Approach"
author: "Jason Sean Ficorilli"
format: html
editor: visual
---

# Introduction

This Quarto markdown file contains the code associated with the paper "Predicting Healthcare Expenditures for Individuals with Breast Cancer: Considering Chronic Comorbidities within a Two-Stage Machine Learning Approach." Several components of this paper's methodology and code implementation are derived from Wu et al. \[1\] and the paper's corresponding R code files (accessible at https://github.com/wuziyueemory/Two-stage-SuperLearner/tree/main/MEPS%20data%20analysis). This code primarily uses data from the Medical Expenditure Panel Survey (MEPS) Household Component Full Year Consolidated data files (years 2015, 2017, 2021, and 2023), which is managed by the U.S. Agency for Healthcare Research and Quality (AHRQ).

Note: the "2023_MEPS_HC_FYC.xlsx" file from this project's GitHub repository MUST be in the same file path as this file to be used in the below first code block. It can be found and downloaded directly here: https://github.com/jficorilli/DS_340W_Project/blob/main/Project%20Implementation/2023_MEPS_HC_FYC.xlsx

# Loading Packages and MEPS HC FYC Data Files

```{r, message = FALSE, warning = FALSE, results='hide'}
# # Install packages from CRAN (if not already installed)
# install.packages("survey")
# install.packages("foreign")
# install.packages("haven")
# install.packages("tidyverse")
# install.packages("scales")
# install.packages("readxl")
# install.packages("kableExtra")
# install.packages("quadprog")
# install.packages("ggcorrplot")
# install.packages("cowplot")
# install.packages("haven")
# install.packages("shapviz")
# install.packages("fastshap")
# 
# # Install packages from GitHub (if not already installed)
# install.packages("devtools")
# library(devtools)
# 
# devtools::install_github("wuziyueemory/twostageSL")
# devtools::install_github("e-mitchell/meps_r_pkg/MEPS")


library(survey)
library(foreign) 
library(haven)
library(tidyverse)
library(scales)
library(MEPS)
library(readxl)
library(twostageSL)
library(kableExtra)
library(quadprog)
library(ggcorrplot)
library(cowplot)
library(haven)
library(shapviz)
library(fastshap)

fyc2023 <- read_xlsx("2023_MEPS_HC_FYC.xlsx") # The 2023 MEPS HC FYC file is sourced from the MEPS HC FYC website, as it was not accessible through the "read_MEPS" function in the "MEPS" package as of 11/13/2025
fyc2021 <- read_MEPS(year = 2021, type = "FYC")
fyc2017 <- read_MEPS(year = 2017, type = "FYC")
fyc2015 <- read_MEPS(year = 2015, type = "FYC")
```

## Creating Expenditure Price Adjustment Vectors based on AHRQ Guidelines

The following total expenditures and total out-of-pocket (OOP) expenditures adjustment vectors were sourced from the AHRQ's guidelines for pooling multiple years of MEPS HC FYC data (see https://meps.ahrq.gov/about_meps/Price_Index.shtml#t3a3), with Personal Consumption Expenditure Health from the Bureau of Economic Analysis being used for adjusting total expenditures and the Consumer Price Index for Medical Care being used for adjusting total OOP expenditures. These values were adjusted into 2023 dollars (the most recent year of data considered) and were used from the AHRQ guidelines website (https://meps.ahrq.gov/about_meps/Price_Index.shtml#t3a3).

```{r}
pce_h_2015_2017_2021_2023 <- c(96.830, 100.0, 107.571, 113.119)

cpi_m_2015_2017_2021_2023 <- c(446.752, 475.322, 525.276, 549.084)
```

# Defining Custom Function(s)

A custom scaled version of squared error loss to handle large matrices and outlier values. This was initially implemented by Wu et al. \[1\] as it is present in the two-stage SL model architecture and necessary to derive prediction values for the one-stage SL model and two-stage discrete SL from the two-stage SL results (accessible as a part of https://github.com/wuziyueemory/Two-stage-SuperLearner/blob/main/MEPS%20data%20analysis/twostageSL.R). See the supplementary file in the online publication of Wu et al. \[1\] for additional commentary and justification. The code comments in the following code block follow the conventions used by Wu et al. \[1\].

```{r}
method.CC_LS.scale <- function() {
  computeCoef = function(Z, Y, libraryNames, verbose,
                         obsWeights=rep(1, length(Y)),
                         errorsInLibrary = NULL, ...) {
    # compute cvRisk
    cvRisk <- apply(Z, 2, function(x) mean(obsWeights*(x-Y)^2))
    names(cvRisk) <- libraryNames
    # compute coef
    compute <- function(x, y, wt=rep(1, length(y))) {
      wX <- sqrt(wt) * x
      wY <- sqrt(wt) * y
      D <- crossprod(wX)
      d <- crossprod(wX, wY)
      A <- cbind(rep(1, ncol(wX)), diag(ncol(wX)))
      bvec <- c(1, rep(0, ncol(wX)))
      sc <- norm(D,"2")
      # scale D matrix & d vector to aviod inconsistent constraints
      fit <- quadprog::solve.QP(Dmat=D/sc, dvec=d/sc, Amat=A, bvec=bvec, meq=1,
                                factorized = F)
      invisible(fit)
    }
    modZ <- Z
    # check for columns of all zeros. assume these correspond
    # to errors that SuperLearner sets equal to 0. not a robust
    # solution, since in theory an algorithm could predict 0 for
    # all observations (e.g., SL.mean when all Y in training = 0)
    naCols <- which(apply(Z, 2, function(z){ all(z == 0 ) }))
    anyNACols <- length(naCols) > 0
    if(anyNACols){
      # if present, throw warning identifying learners
      warning(paste0(paste0(libraryNames[naCols],collapse = ", "), " have NAs.",
                     "Removing from super learner."))
    }
    # check for duplicated columns
    # set a tolerance level to avoid numerical instability
    tol <- 8
    dupCols <- which(duplicated(round(Z, tol), MARGIN = 2))
    anyDupCols <- length(dupCols) > 0
    if(anyDupCols){
      # if present, throw warning identifying learners
      warning(paste0(paste0(libraryNames[dupCols],collapse = ", "),
                     " are duplicates of previous learners.",
                     " Removing from super learner."))
    }
    # remove from Z if present
    if(anyDupCols | anyNACols){
      rmCols <- unique(c(naCols,dupCols))
      modZ <- Z[,-rmCols]
    }
    # compute coefficients on remaining columns
    fit <- compute(x = modZ, y = Y, wt = obsWeights)
    coef <- fit$solution
    if (anyNA(coef)) {
      warning("Some algorithms have weights of NA, setting to 0.")
      coef[is.na(coef)] = 0
    }
    # add in coefficients with 0 weights for algorithms with NAs
    if(anyDupCols | anyNACols){
      ind <- c(seq_along(coef), rmCols - 0.5)
      coef <- c(coef, rep(0, length(rmCols)))
      coef <- coef[order(ind)]
    }
    # Set very small coefficients to 0 and renormalize.
    coef[coef < 1.0e-4] <- 0
    coef <- coef / sum(coef)
    if(!sum(coef) > 0) warning("All algorithms have zero weight", call. = FALSE)
    list(cvRisk = cvRisk, coef = coef, optimizer = fit)
  }
  
  computePred = function(predY, coef, ...) {
    predY %*% matrix(coef)
  }
  out <- list(require = "quadprog",
              computeCoef = computeCoef,
              computePred = computePred)
  invisible(out)
}

```

A custom version of the two-stage model prediction function in "predict.twostageSL.R" (https://github.com/wuziyueemory/twostageSL/blob/master/R/predict.twostageSL.R) from the "twostageSL" R package. This custom version makes only one change: changing mentions of "object\$fitLibrary\$stage.single\[\[mm\]\]" to "object\$fitLibrary\$stage.sinlge\[\[mm\]\]". This change was made to match a typo in the "twostageSL()" model's source code, which stopped the prediction function from running in its current state. All other function code and comments are kept from the source code besides the one change. This function is not used for making model predictions in this code (as the two-stage model was retrained each time for prediction to keep coefficients for extracting one-stage and discrete two-stage predictions), but was used in the creation of SHAP values for total expenditures and total out-of-pocket expenditures prediction.

```{r}
predict_twostageSL_custom <- function(object, newdata, X = NULL, Y = NULL,
                                 onlySL = FALSE, ...) {
  if (missing(newdata)) {
    out <- list(pred = object$SL.predict, library.predict = object$library.predict)
    return(out)
  }
  if (!object$control$saveFitLibrary) {
    stop("This SuperLearner fit was created using control$saveFitLibrary = FALSE, so new predictions cannot be made.")
  }
  N <- dim(object$Z)[[1]]
  k.stage1 <- object$library.Num$stage1
  k.stage2 <- object$library.Num$stage2
  k.singlestage <- object$library.Num$stage.single
  if (onlySL) {
    whichLibrary <- which(object$coef > 0)
    # predictions for only algorithm with coef > 0
    predY.stage1 <- matrix(NA, nrow = nrow(newdata), ncol = k.stage1)
    predY.stage2 <- matrix(NA, nrow = nrow(newdata), ncol = k.stage2)
    predY.singlestage <- matrix(NA, nrow = nrow(newdata), ncol = k.singlestage)
    # stage 1
    for (mm in seq(k.stage1)) {
      newdataMM.stage1 <- subset(newdata,
                                 select = object$whichScreen$stage1[object$SL.library$library$twostage[mm*k.stage2, 2], ])
      family.stage1 <- object$family$stage1
      XMM.stage1 <- if (is.null(X)) {
        NULL
      } else {
        subset(X, select = object$whichScreen$stage1[object$SL.library$library$twostage[mm*k.stage2, 2], ])
      }
      predY.stage1[, mm] <- do.call('predict', list(object = object$fitLibrary$stage1[[mm]],
                                                    newdata = newdataMM.stage1,
                                                    family = family.stage1,
                                                    X = XMM.stage1,
                                                    Y = as.numeric(Y==0)))

    }
    # stage 2
    Y.p <- Y[Y>0]
    for (mm in seq(k.stage2)) {
      newdataMM.stage2 <- subset(newdata,
                                 select = object$whichScreen$stage2[object$SL.library$library$twostage[mm, 3], ])
      family.stage2 <- object$family$stage2
      XMM.stage2 <- if (is.null(X)) {
        NULL
      } else {
        dat <- cbind(X,Y)
        X.p <- dat[dat$Y>0,-ncol(dat)]
        subset(X.p, select = object$whichScreen$stage2[object$SL.library$library$twostage[mm, 3], ])
      }
      predY.stage2[, mm] <- do.call('predict', list(object = object$fitLibrary$stage2[[mm]],
                                                    newdata = newdataMM.stage2,
                                                    family = family.stage2,
                                                    X = XMM.stage2,
                                                    Y = Y.p))
    }
    # single stage
    for (mm in seq(k.singlestage)) {
      newdataMM.singestage <- subset(newdata,
                                     select = object$whichScreen$single.stage[object$SL.library$library$singlestage[mm, 2], ])
      family.singlestage <- object$family$stage.single
      XMM.singlestage <- if (is.null(X)) {
        NULL
      } else {
        subset(X, select = object$whichScreen$single.stage[object$SL.library$library$singlestage[mm, 2], ])
      }
      predY.singlestage[, mm] <- do.call('predict', list(object = object$fitLibrary$stage.single[[mm]],
                                                         newdata = newdataMM.singestage,
                                                         family = family.singlestage,
                                                         X = XMM.singlestage,
                                                         Y = Y))
    }
    # get prediction for 2-stage model
    predY <- NULL
    for (i in 1:k.stage1){
      for (j in 1:k.stage2){
        pred <- (1-predY.stage1[,i])*predY.stage2[,j]
        predY <- cbind(predY,pred)
      }
    }
    # combine with prediction from singe-stage model
    predY <- cbind(predY,predY.singlestage)
    colnames(predY) <- object$libraryNames
    getPred <- object$method$computePred(predY = predY, coef = object$coef,
                                         control = object$control)
    predY.onlySL <- predY[,whichLibrary]
    out <- list(pred = getPred, library.predict = predY.onlySL)
    class(out) <- "predict.twostageSL"
    out
  } else {
    # predictions for all algorithms
    predY.stage1 <- matrix(NA, nrow = nrow(newdata), ncol = k.stage1)
    predY.stage2 <- matrix(NA, nrow = nrow(newdata), ncol = k.stage2)
    predY.singlestage <- matrix(NA, nrow = nrow(newdata), ncol = k.singlestage)
    # stage 1
    for (mm in seq(k.stage1)) {
      newdataMM.stage1 <- subset(newdata,
                          select = object$whichScreen$stage1[object$SL.library$library$twostage[mm*k.stage2, 2], ])
      family.stage1 <- object$family$stage1
      XMM.stage1 <- if (is.null(X)) {
        NULL
      } else {
        subset(X, select = object$whichScreen$stage1[object$SL.library$library$twostage[mm*k.stage2, 2], ])
      }
      predY.stage1[, mm] <- do.call('predict', list(object = object$fitLibrary$stage1[[mm]],
                                             newdata = newdataMM.stage1,
                                             family = family.stage1,
                                             X = XMM.stage1,
                                             Y = as.numeric(Y==0)))

    }
    # stage 2
    Y.p <- Y[Y>0]
    for (mm in seq(k.stage2)) {
      newdataMM.stage2 <- subset(newdata,
                                 select = object$whichScreen$stage2[object$SL.library$library$twostage[mm, 3], ])
      family.stage2 <- object$family$stage2
      XMM.stage2 <- if (is.null(X)) {
        NULL
      } else {
        dat <- cbind(X,Y)
        X.p <- dat[dat$Y>0,-ncol(dat)]
        subset(X.p, select = object$whichScreen$stage2[object$SL.library$library$twostage[mm, 3], ])
      }
      predY.stage2[, mm] <- do.call('predict', list(object = object$fitLibrary$stage2[[mm]],
                                                    newdata = newdataMM.stage2,
                                                    family = family.stage2,
                                                    X = XMM.stage2,
                                                    Y = Y.p))
    }
    # single stage
    for (mm in seq(k.singlestage)) {
      newdataMM.singestage <- subset(newdata,
                          select = object$whichScreen$single.stage[object$SL.library$library$singlestage[mm, 2], ])
      family.singlestage <- object$family$stage.single
      XMM.singlestage <- if (is.null(X)) {
        NULL
      } else {
        subset(X, select = object$whichScreen$single.stage[object$SL.library$library$singlestage[mm, 2], ])
      }
      predY.singlestage[, mm] <- do.call('predict', list(object = object$fitLibrary$stage.sinlge[[mm]], # changed line to fix error in function's GitHub repository code (author wrote "...$stage.single" when it should match a typo "...$stage.sinlge" based on the model's code)
                                             newdata = newdataMM.singestage,
                                             family = family.singlestage,
                                             X = XMM.singlestage,
                                             Y = Y))
    }
    # get prediction for 2-stage model
    predY <- NULL
    for (i in 1:k.stage1){
      for (j in 1:k.stage2){
        pred <- (1-predY.stage1[,i])*predY.stage2[,j]
        predY <- cbind(predY,pred)
      }
    }
    # combine with prediction from singe-stage model
    predY <- cbind(predY,predY.singlestage)
    colnames(predY) <- object$libraryNames
    getPred <- object$method$computePred(predY = predY, coef = object$coef,
                                         control = object$control)
    out <- list(pred = getPred, library.predict = predY)
  }
  # class(out) <- "predict.twostageSL"
  # cat("Predictions from the two-stage Super Learner:  pred \n\nPrediction for all algorithms in the library:  library.predict\n")
  out
}
```

A two-step function for creating a comparative results data frame for a two-stage SL model object ("tssl_object") and a one-stage SL, discrete two-stage SL, and individual learner algorithms derived from the two-stage SL on newY (unseen validation/testing Y). "newY" must be a vector and match the data set containing the newX used to train the two-stage SL model object. The first step computes predictions for the one-stage SL, discrete two-stage SL, and individual learner algorithms (defined in "library.single()" in the "tssl_object" model definition), primarily following code from Wu et al. [1]. The second step computes MSE, RMSE, R2, and MAE metrics for all comparable models' predictions against "newY".

```{r}
get_tssl_results_df <- function(tssl_object, newY){
  # Step 1. Determine results from one-stage SL and discrete two-stage SL (following code from Wu et al. [1])

  # construct one-stage superlearner

  # extract onestage matrix z1
  z1 <- tssl_object$Z[,(dim(tssl_object$Z)[2] - nrow(tssl_object$SL.library$library$singlestage[1]) + 1):dim(tssl_object$Z)[2]]
  onestagename <- colnames(tssl_object$library.predict[,(dim(tssl_object$Z)[2] - nrow(tssl_object$SL.library$library$singlestage[1]) + 1):dim(tssl_object$Z)[2]])
  
  # get optimum weights for each algorithm in one-stage
  getCoef <- method.CC_LS.scale()$computeCoef(Z=z1,Y=Train$TOTEXP,libraryNames=onestagename,
                                                verbose=FALSE)
  coef_onestage <- getCoef$coef
  
  # Prediction for each algorithm in one-stage superlearner
  predY_onestage <- tssl_object$library.predict[,(dim(tssl_object$Z)[2] - nrow(tssl_object$SL.library$library$singlestage[1]) + 1):dim(tssl_object$Z)[2]]
  
  # Compute onestage superlearner predictions on newX.
  onestage_pred <- tssl_object$method$computePred(predY = predY_onestage, coef = coef_onestage, 
                                                     control = tssl_object$control)
  # get discrete two-stage superlearner
  discrete_pred <- tssl_object$library.predict[,which.min(tssl_object$cvRisk)]
  
  
  # Step 2. Creating results df

  # Store column names
  column_names <- c(colnames(tssl_object$library.predict), "Two-Stage Super Learner", "One-Stage Super Learner", "Discrete Two-Stage Super Learner")
  
  # Remove indicator that all variables were used in the models/no screening algorithms were used ("_All") and fix model names
  column_names <- sapply(column_names, function(x){gsub("_All", "", x)})
  column_names <- sapply(column_names, function(x){gsub("SL.", "", x)})
  column_names <- sapply(column_names, function(x){gsub("xgboost", "XGBoost", x)})
  column_names <- sapply(column_names, function(x){gsub("randomForest", "Random Forest", x)})
  column_names <- sapply(column_names, function(x){gsub("glmnet", "LASSO Regression", x)})
  column_names <- sapply(column_names, function(x){gsub("glm", "Logistic Regression", x)})
  column_names <- sapply(column_names, function(x){gsub("lm", "Linear Regression", x)})
  column_names <- sapply(column_names, function(x){gsub("S1: LASSO Regression", "S1: LASSO Linear Regression", x)})
  column_names <- sapply(column_names, function(x){gsub("S2: LASSO Regression", "S2: LASSO Logistic Regression", x)})
  column_names <- sapply(column_names, function(x){gsub("Single: LASSO Regression", "Single: LASSO Linear Regression", x)})
  
  # Add MSE, RMSE, R2, and MAE values for each algorithm variation in two-stage SL
  results_df <- data.frame(apply(tssl_object$library.predict, 2, function(pred) {
    c(
      mean((newY - pred)^2),
      sqrt(mean((newY - pred)^2)),
      1 - (sum((newY - pred)^2) / sum((newY - mean(newY))^2)),
      mean(abs(newY - pred))
    )
  })
  )
  
  # Add two-stage SL results
  results_df[["two_stage_SL_preds"]] <- c(
      mean((newY - tssl_object$SL.predict)^2),
      sqrt(mean((newY - tssl_object$SL.predict)^2)),
      1 - (sum((newY - tssl_object$SL.predict)^2) / sum((newY - mean(newY))^2)),
      mean(abs(newY - tssl_object$SL.predict))
    )
  
  
  # Add one-stage SL results
  results_df[["one_stage_SL_preds"]] <- c(
      mean((newY - onestage_pred)^2),
      sqrt(mean((newY - onestage_pred)^2)),
      1 - (sum((newY - onestage_pred)^2) / sum((newY - mean(newY))^2)),
      mean(abs(newY - onestage_pred))
    )
  
  # Add discrete two-stage SL results
  results_df[["discrete_two_stage_SL_preds"]] <- c(
      mean((newY - discrete_pred)^2),
      sqrt(mean((newY - discrete_pred)^2)),
      1 - (sum((newY - discrete_pred)^2) / sum((newY - mean(newY))^2)),
      mean(abs(newY - discrete_pred))
    )
  
  # Label rows
  rownames(results_df) <- c("MSE", "RMSE", "R2", "MAE")
  
  # Transpose the table
  results_df <- data.frame(t(results_df))
  
  # Fix the row names
  rownames(results_df) <- column_names
  
  # Arrange the table by highest R2
  results_df <- results_df %>% arrange(desc(R2))
  
  # Format results in non-scientific notation
  results_df[] <- lapply(results_df, function(x) {
    if (is.numeric(x)) format(x, scientific = FALSE)
    else x
  })
  
  # Return the final df
  return(results_df)
}
```

# Data Preprocessing

## Creating "YEAR" Variables, Standardizing Variable Names Across Years, Adjusting Total and OOP Expenditures to 2023 Dollars (following AHRQ guidelines), then Selecting Relevant Variables

This code section roughly follows the AHRQ-sponsored R guides of pooling MEPS data (https://github.com/HHS-AHRQ/MEPS/blob/master/R/workshop_exercises/exercise_3d.R) available on the AHRQ's GitHub repository page (https://github.com/HHS-AHRQ/MEPS/tree/master/R).

```{r}
# Standardizing 2023 data
fyc2023p <- fyc2023 %>%
  
  # Adding YEAR variable
  mutate(YEAR = 2023) %>%
  
  # Standardizing variable names across years
  rename(
    TOTEXP = TOTEXP23,
    TOTSLF = TOTSLF23,
    AGE_YE = AGE23X,
    POVLEV = POVLEV23,
    INSCOV = INSCOV23,
    DDNWRK = DDNWRK23,
    REGION = REGION23,
    MARRIED = MARRY23X,
    IPDIS = IPDIS23,
    OBTOTV = OBTOTV23,
    RXTOT = RXTOT23,
    DIABDX = DIABDX_M18,
    
    PERWT = PERWT23F
  ) %>%
  
  # Selecting relevant variables
  select(CABREAST, DUPERSID, PANEL, VARSTR, VARPSU, PERWT, AGE_YE, EDUCYR, POVLEV, INSCOV, RACETHX, REGION, DIABDX, HIBPDX, ANGIDX, ARTHDX, ASTHDX, CABLADDR, CACERVIX, CACOLON, CALUNG, CALYMPH, CAMELANO, CASKINDK, CASKINNM, CAUTERUS, CHDDX, CHOLDX, EMPHDX, HIBPDX, MIDX, OHRTDX, RXTOT, STRKDX, MARRIED, IPDIS, OBTOTV, YEAR, TOTEXP, TOTSLF)

# Standardizing 2021 data
fyc2021p <- fyc2021 %>%
  
  # Adding YEAR variable and adjusting expenditures to 2023 dollars 
  mutate(YEAR = 2021,
         TOTEXP21 = TOTEXP21 * (pce_h_2015_2017_2021_2023[4]/pce_h_2015_2017_2021_2023[3]),
         TOTSLF21 = TOTSLF21 * (cpi_m_2015_2017_2021_2023[4]/cpi_m_2015_2017_2021_2023[3])
         ) %>%
  
  # Standardizing variable names across years
  rename(
    TOTEXP = TOTEXP21,
    TOTSLF = TOTSLF21,
    AGE_YE = AGE21X,
    POVLEV = POVLEV21,
    INSCOV = INSCOV21,
    DDNWRK = DDNWRK21,
    REGION = REGION21,
    MARRIED = MARRY21X,
    IPDIS = IPDIS21,
    OBTOTV = OBTOTV21,
    RXTOT = RXTOT21,
    DIABDX = DIABDX_M18,
    
    PERWT = PERWT21F
  ) %>%
  
  select(CABREAST, DUPERSID, PANEL, VARSTR, VARPSU, PERWT, AGE_YE, EDUCYR, POVLEV, INSCOV, RACETHX, REGION, DIABDX, HIBPDX, ANGIDX, ARTHDX, ASTHDX, CABLADDR, CACERVIX, CACOLON, CALUNG, CALYMPH, CAMELANO, CASKINDK, CASKINNM, CAUTERUS, CHDDX, CHOLDX, EMPHDX, HIBPDX, MIDX, OHRTDX, RXTOT, STRKDX, MARRIED, IPDIS, OBTOTV, YEAR, TOTEXP, TOTSLF)

# Standardizing 2017 data
fyc2017p <- fyc2017 %>%
  
  # Adding YEAR variable and adjusting expenditures to 2023 dollars
  mutate(YEAR = 2017,
         TOTEXP17 = TOTEXP17 * (pce_h_2015_2017_2021_2023[4]/pce_h_2015_2017_2021_2023[2]),
         TOTSLF17 = TOTSLF17 * (cpi_m_2015_2017_2021_2023[4]/cpi_m_2015_2017_2021_2023[2])
         ) %>%

  # Standardizing variable names across years
  rename(
    TOTEXP = TOTEXP17,
    TOTSLF = TOTSLF17,
    AGE_YE = AGE17X,
    POVLEV = POVLEV17,
    INSCOV = INSCOV17,
    DDNWRK = DDNWRK17,
    REGION = REGION17,
    MARRIED = MARRY17X,
    IPDIS = IPDIS17,
    OBTOTV = OBTOTV17,
    RXTOT = RXTOT17,

    PERWT = PERWT17F
  ) %>%

  # Selecting relevant variables
  select(CABREAST, DUPERSID, PANEL, VARSTR, VARPSU, PERWT, AGE_YE, EDUCYR, POVLEV, INSCOV, RACETHX, REGION, DIABDX, HIBPDX, ANGIDX, ARTHDX, ASTHDX, CABLADDR, CACERVIX, CACOLON, CALUNG, CALYMPH, CAMELANO, CASKINDK, CASKINNM, CAUTERUS, CHDDX, CHOLDX, EMPHDX, HIBPDX, MIDX, OHRTDX, RXTOT, STRKDX, MARRIED, IPDIS, OBTOTV, YEAR, TOTEXP, TOTSLF)


# Standardizing 2015 data
fyc2015p <- fyc2015 %>%
  
  # Adding YEAR variable and adjusting expenditures to 2023 dollars
  mutate(YEAR = 2015,
         TOTEXP15 = TOTEXP15 * (pce_h_2015_2017_2021_2023[4]/pce_h_2015_2017_2021_2023[1]),
         TOTSLF15 = TOTSLF15 * (cpi_m_2015_2017_2021_2023[4]/cpi_m_2015_2017_2021_2023[1])
         ) %>%

  # Standardizing variable names across years
  rename(
    TOTEXP = TOTEXP15,
    TOTSLF = TOTSLF15,
    AGE_YE = AGE15X,
    POVLEV = POVLEV15,
    INSCOV = INSCOV15,
    DDNWRK = DDNWRK15,
    REGION = REGION15,
    MARRIED = MARRY15X,
    IPDIS = IPDIS15,
    OBTOTV = OBTOTV15,
    RXTOT = RXTOT15,

    PERWT = PERWT15F
  ) %>%

  # Selecting relevant variables
  select(CABREAST, DUPERSID, PANEL, VARSTR, VARPSU, PERWT, AGE_YE, EDUCYR, POVLEV, INSCOV, RACETHX, REGION, DIABDX, HIBPDX, ANGIDX, ARTHDX, ASTHDX, CABLADDR, CACERVIX, CACOLON, CALUNG, CALYMPH, CAMELANO, CASKINDK, CASKINNM, CAUTERUS, CHDDX, CHOLDX, EMPHDX, HIBPDX, MIDX, OHRTDX, RXTOT, STRKDX, MARRIED, IPDIS, OBTOTV, YEAR, TOTEXP, TOTSLF)

```

## Stack data and pool based on breast cancer subpopulation of interest

Continuing from above, this code section roughly follows the AHRQ-sponsored R guides of pooling MEPS data (https://github.com/HHS-AHRQ/MEPS/blob/master/R/workshop_exercises/exercise_3d.R) available on the AHRQ's GitHub page (https://github.com/HHS-AHRQ/MEPS/tree/master/R). This also includes dividing the PERWT (per-person-weight) variable by the number of annual data files being pooled to ensure accurate estimates. Since this file does not create nationally representative estimates from the MEPS HC FYC data, it is optional to use the "poolwt" variable, as the results are focused solely on the survey samples. Due to this, and in the interest of preserving sample size, the variable will be dropped from the sample and not used in the modeling process. This avoided the need to drop observations that had weights of 0.

```{r, warning = FALSE}
pool <- bind_rows(fyc2023p, fyc2021p, fyc2017p, fyc2015p) %>%
  mutate(
    # Dividing PERWT by number of years (# of yearly data files used) to create pooling weight
    poolwt = PERWT / 4,
    subpop = (AGE_YE >= 18 
              & CABREAST == 1
              )
  )
```

## Filtering Pooled and Data Feature Engineering

The below filtering of the pooled dataset ensures that the correct population (individuals who were ever diagnosed with breast cancer) are being considered, as well as having valid entries for all relevant regional location, family income (as a % of poverty line), marriage status (as of the 12/31/20## at the end of each data file's respective year) and chronic cormobidity covariates. See the codebook for the 2023 MEPS HC FYC data file (https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_codebook.jsp?PUFId=H251) for translations of variable names used in the 2023 edition, as well as which values could be considered null or invalid for each variable. Also, see the MEPS HC variable explorer (https://datatools.ahrq.gov/meps-hc/) for more information on variable name changes across years. The filtering process reduced the total pooled observation count from N = 114,562 to N = 1,394.

```{r}
pool_filtered <- pool %>%
  filter(
    subpop == TRUE # individuals that reported ever being diagnosed with breast cancer (female only population)
    & REGION >= 1 # valid value for region
    & POVLEV >= 0 # valid value for family income as a % of poverty line
    & MARRIED >= 1 # marriage status
    & EDUCYR >= 0 # valid value for years of education when first entered MEPS
    
    # Ensuring valid values (being eligible for question consideration in MEPS HC and not "Don't Know" response) for ever being diagnosed with:
    & DIABDX >= 1 # diabetes
    & HIBPDX >= 1 # high blood pressure
    & ANGIDX >= 1 # angina
    & ARTHDX >= 1 # arthritis
    & ASTHDX >= 1 # asthma
    & CABLADDR >= 1 # bladder cancer
    & CACERVIX >= 1 # cervical cancer
    & CACOLON >= 1 # colon cancer
    & CALUNG >= 1 # lung cancer
    & CALYMPH >= 1 # arthritis
    & CAMELANO >= 1 # cancer - skin melanoma
    & CASKINDK >= 1 # cancer - unknown type
    & CASKINNM >= 1 # cancer - skin non-melanoma
    & CAUTERUS >= 1 # uterine cancer
    & CHDDX >= 1 # coronary heart disease
    & CHOLDX >= 1 # high cholesterol
    & EMPHDX >= 1 # cancer emphysema
    & HIBPDX >= 1 # high blood pressure
    & MIDX >= 1 # heart attack (myocardial infarction)
    & STRKDX >= 1 # ever having a stroke
    ) %>%
  
  # Removing non-needed variables
  select(!c("CABREAST", "DUPERSID", "PANEL", "VARSTR", "VARPSU", "PERWT", "subpop", "poolwt")) %>%
  
  # Feature engineering - one-hot encoding all categorical features
  mutate(
    
    # One-hot encoding all chronic comorbidity covariates
    DIABDX = if_else(DIABDX == 1, 1, 0),
    HIBPDX = if_else(HIBPDX == 1, 1, 0),
    ANGIDX = if_else(ANGIDX == 1, 1, 0),
    ARTHDX = if_else(ARTHDX == 1, 1, 0),
    ASTHDX = if_else(ASTHDX == 1, 1, 0),
    CABLADDR = if_else(CABLADDR == 1, 1, 0),
    CACERVIX = if_else(CACERVIX == 1, 1, 0),
    CACOLON = if_else(CACOLON == 1, 1, 0),
    CALUNG = if_else(CALUNG == 1, 1, 0),
    CALYMPH = if_else(CALYMPH == 1, 1, 0),
    CAMELANO = if_else(CAMELANO == 1, 1, 0),
    CASKINDK = if_else(CASKINDK == 1, 1, 0),
    CASKINNM = if_else(CASKINNM == 1, 1, 0),
    CAUTERUS = if_else(CAUTERUS == 1, 1, 0),
    CHDDX = if_else(CHDDX == 1, 1, 0),
    CHOLDX = if_else(CHOLDX == 1, 1, 0),
    EMPHDX = if_else(EMPHDX == 1, 1, 0),
    HIBPDX = if_else(HIBPDX == 1, 1, 0),
    MIDX = if_else(MIDX == 1, 1, 0),
    STRKDX = if_else(STRKDX == 1, 1, 0),
    OHRTDX = if_else(OHRTDX == 1, 1, 0),
    
    # Will leave 2016 out as reference year
    IF_2023 = if_else(YEAR == 2023, 1, 0),
    IF_2021 = if_else(YEAR == 2021, 1, 0),
    IF_2017 = if_else(YEAR == 2017, 1, 0),
    IF_2015 = if_else(YEAR == 2015, 1, 0),
    
    # Will leave out non-Hispanic white as reference race
    RACETHX_HISPANIC = if_else(RACETHX == 1, 1, 0),
    RACETHX_WHITE = if_else(RACETHX == 2, 1, 0),
    RACETHX_BLACK = if_else(RACETHX == 3, 1, 0),
    RACETHX_ASIAN = if_else(RACETHX == 4, 1, 0),
    RACETHX_OTHER = if_else(RACETHX == 5, 1, 0),
    
    # Will leave out South as reference region
    REGION_NE = if_else(REGION == 1, 1, 0),
    REGION_MW = if_else(REGION == 2, 1, 0),
    REGION_S = if_else(REGION == 3, 1, 0),
    REGION_W = if_else(REGION == 4, 1, 0),
    
    # Will leave out "NEVER MARRIED" as reference
    IF_MARRIED = if_else(MARRIED == 1, 1, 0),
    IF_WIDOWED = if_else(MARRIED == 2, 1, 0),
    IF_DIVORCED = if_else(MARRIED == 3, 1, 0),
    IF_SEPARATED = if_else(MARRIED == 4, 1, 0),
    IF_NEVER_MARRIED = if_else(MARRIED == 5, 1, 0),
    
    # Will leave out "UNINSURED" as reference
    PRIVATE_INS_YR = if_else(INSCOV == 1, 1, 0),
    PUBLIC_INS_YR = if_else(INSCOV == 2, 1, 0),
    UNINSURED_YR = if_else(INSCOV == 3, 1, 0),
  ) %>% 
  
  # Removing variable labels and ensuring numeric variable types (code line created with ChatGPT)
  mutate(across(where(is.labelled), ~as.numeric(zap_labels(.x)))) %>%
  
  # Removing variables used in feature engineering
  select(!c("RACETHX", "REGION", "MARRIED", "INSCOV", "YEAR"))

```

# Summary Statistics

## Quantitative Variables

```{r}
# Creating data frame of quantitative variable summary stats
quant_summary_stats_df <- data.frame(
  summary(pool_filtered %>% 
            select(TOTEXP, TOTSLF, AGE_YE, EDUCYR, POVLEV, IPDIS, OBTOTV, RXTOT), digits = 8
          )
  ) %>%
  select(-c("Var1")) %>%
  separate_wider_delim(
    cols = Freq, delim = ":", names = c("Measure", "Amount")) %>%
  rename("Name" = "Var2")


# Using measurements as column names
quant_summary_stats_df <- quant_summary_stats_df %>%
  pivot_wider(names_from = Measure, values_from = Amount)

# Fixing measurement column names
quant_summary_stats_df <- data.frame(apply(quant_summary_stats_df, 2, str_remove_all, " ")) %>%
  rename("Min" = "Min....") %>%
  rename("1st Quartile" = "X1st.Qu.") %>%
  rename("Median" = "Median.") %>%
  rename("Mean" = "Mean...") %>%
  rename("3rd Quartile" = "X3rd.Qu.") %>%
  rename("Max" = "Max....")
```

```{r}
# Showing table
quant_summary_stats_df %>%
  kable() %>%
  kable_classic(
    full_width = F,
    latex_options = "scale_down"
    )
```

```{r}
# Locally save the quantitative summary statistics table
# write.csv(quant_summary_stats_df, file = "quant_summary_stats.csv")
```

## Qualitative Variables

```{r}
# Creating data frame of quantitative variable summary stats
qual_summary_stats_df <- data.frame(
  variable = colnames(pool_filtered %>% select(!c("TOTEXP", "TOTSLF", "AGE_YE", "EDUCYR", "POVLEV", "IPDIS", "OBTOTV", "RXTOT"))),
  Yes = apply(pool_filtered %>% select(!c("TOTEXP", "TOTSLF", "AGE_YE", "EDUCYR", "POVLEV", "IPDIS", "OBTOTV", "RXTOT")), 2, sum),
  No = nrow(pool_filtered) - apply(pool_filtered %>% select(!c("TOTEXP", "TOTSLF", "AGE_YE", "EDUCYR", "POVLEV", "IPDIS", "OBTOTV", "RXTOT")), 2, sum)
) %>%
  pivot_longer(
    cols = c("Yes", "No"),
    names_to = "have_condition",
    values_to = "N"
  ) %>%
  mutate(Total_percent = N / nrow(pool_filtered))
```

```{r}
# Showing table
qual_summary_stats_df %>%
  kable() %>%
  kable_classic(
    full_width = F,
    latex_options = "scale_down"
    )
```

```{r}
# Locally save the qualitative summary statistics table
# write.csv(qual_summary_stats_df, file = "qual_summary_stats.csv")
```

## Finalize Pooled Data File

```{r}
# Drop reference groups
pool_final <- pool_filtered %>% select(!c("IF_2015", "RACETHX_WHITE", "REGION_S", "IF_NEVER_MARRIED", "UNINSURED_YR"))
```

```{r}
# Locally save the final pooled data file
# write.csv(pool_final, file = "pool_final.csv")
```

# Exploratory Data Analysis

```{r}
# Correlation heatmap of continuous variables
pool_final %>% 
  select(c("TOTEXP", "TOTSLF", "AGE_YE", "EDUCYR", "POVLEV", "IPDIS", "OBTOTV", "RXTOT")) %>%
  cor() %>%
  ggcorrplot(lab = TRUE)
```

```{r, message=FALSE}
# Distribution of two target variables
plot_grid(ggplot(pool_final, aes(x = TOTEXP)) + geom_histogram(fill = "lightblue", color = "black", binwidth = 7500) + labs(x = "Total Healthcare Expenditures", y = "Count") + theme_bw() + scale_x_continuous(labels = label_dollar()),
          ggplot(pool_final, aes(x = TOTSLF)) + geom_histogram(fill = "lightblue", color = "black", binwidth = 3000) + labs(x = "Total Out-of-Pocket Healthcare Expenditures", y = "Count") + theme_bw() + scale_x_continuous(labels = label_dollar()))
```

```{r}
# Identify any zero-inflation
data.frame(target_variable = c("TOTEXP", "TOTSLF"),
           zero_values_count = c(nrow(pool_final %>% filter(TOTEXP == 0)), nrow(pool_final %>% filter(TOTSLF == 0)))
) %>%
  mutate(Total_Percent = zero_values_count / nrow(pool_final))
```

# Two-Stage Super Learner Modeling Implementation

## Creating Training/Validation/Testing Split

```{r}
# Create train, validation, and test sets using a 70%/15%/15% split
set.seed(321) # setting random seed for reproducibility

# 70/30 initial train/test split
train_ind <- sample(
  1:nrow(pool_final), 
  floor(0.7 * nrow(pool_final))
)

Train <- pool_final[train_ind, ]
Test_temp <- pool_final[-train_ind, ]


# Creating validation set from testing set to create 70%/15%/15% split
train_ind <- sample(
  1:nrow(Test_temp), 
  floor(0.5 * nrow(Test_temp))
)

Validation <- Test_temp[train_ind, ]
Test <- Test_temp[-train_ind, ]

```

```{r}
# Locally save the train, validation, and test files
# write.csv(Train, file = "Train.csv")
# write.csv(Validation, file = "Validation.csv")
# write.csv(Test, file = "Test.csv")
```

## Total Expenditures Prediction

### Validation Set Predictions

The below code block is a fully verbose (verbose = TRUE) running of the two-stage SL model on the validation set for total expenditures prediction. Verbose is set to TRUE to show the complete output for this one instance. Since the result is very long and not necessary to run the model, verbose will be set to FALSE for the test set and predicting OOP expenditures, as well as YAML parameters suppressing some messages from the code output.

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown for this code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the validation set.  

```{r}
# Reset random seed
set.seed(321)

# Train model on training data and predict on validation set 
totexp_tssl <- twostageSL(
  Y = Train$TOTEXP,
  X = Train %>% select(!c("TOTEXP", "TOTSLF")),
  newX = Validation %>% select(!c("TOTEXP", "TOTSLF")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = TRUE
)
```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner)
data.frame(totexp_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
validation_results_df_totexp <- get_tssl_results_df(tssl_object = totexp_tssl, newY = Validation$TOTEXP)
```

```{r}
# Locally save the results table
# write.csv(validation_results_df_totexp, file = "validation_results_df_totexp.csv")
```


### Test Set Predictions

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown above for the TOTEXP validation set prediction code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the testing set. Warnings and excess output have been suppressed using YAML parameters and verbose = FALSE in the model definition.

```{r, error = FALSE, message = FALSE, warning = FALSE}
# Re-training model to predict on unseen test set (no working native "predict()" function)

# Reset random seed
set.seed(321)

# Re-train model and predict on test set
totexp_tssl <- twostageSL(
  Y = Train$TOTEXP,
  X = Train %>% select(!c("TOTEXP", "TOTSLF")),
  newX = Test %>% select(!c("TOTEXP", "TOTSLF")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = FALSE
)
```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner -> should match the earlier coefficients from predicting on the validation set)
data.frame(totexp_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
final_results_df_totexp <- get_tssl_results_df(tssl_object = totexp_tssl, newY = Test$TOTEXP)
```

```{r}
# Locally save the results table
# write.csv(final_results_df_totexp, file = "final_results_df_totexp.csv")
```

## Total OOP Expenditures Prediction

### Validation Set Predictions

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown above for the TOTEXP validation set prediction code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the validation set. Warnings and excess output have been suppressed using YAML parameters and verbose = FALSE in the model definition.

```{r, message = FALSE, warning = FALSE}
# Reset random seed
set.seed(321)

# Train model on training data and predict on validation set 
totslf_tssl <- twostageSL(
  Y = Train$TOTSLF,
  X = Train %>% select(!c("TOTEXP", "TOTSLF")),
  newX = Validation %>% select(!c("TOTEXP", "TOTSLF")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = FALSE
)
```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner)
data.frame(totslf_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
validation_results_df_totslf <- get_tssl_results_df(tssl_object = totslf_tssl, newY = Validation$TOTSLF)
```

```{r}
# Locally save the results table
# write.csv(validation_results_df_totslf, file = "validation_results_df_totslf.csv")
```

### Test Set Predictions

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown above for the TOTEXP validation set prediction code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the testing set. Warnings and excess output have been suppressed using YAML parameters and verbose = FALSE in the model definition.

```{r,  message = FALSE, warning = FALSE}
# Reset random seed
set.seed(321)

# Re-training model to predict on unseen test set (no working native "predict()" function) 
totslf_tssl <- twostageSL(
  Y = Train$TOTSLF,
  X = Train %>% select(!c("TOTEXP", "TOTSLF")),
  newX = Test %>% select(!c("TOTEXP", "TOTSLF")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = FALSE
)
```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner -> should match the earlier coefficients from predicting on the validation set)
data.frame(totslf_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
final_results_df_totslf <- get_tssl_results_df(tssl_object = totslf_tssl, newY = Test$TOTSLF)
```

```{r}
# Locally save the results table
# write.csv(final_results_df_totslf, file = "final_results_df_totslf.csv")
```

## View Final Results Tables

The below tables are the main results tables from this project.

```{r}
final_results_df_totexp %>% View()
```

```{r}
final_results_df_totslf %>% View()
```

# SHAP Values Implementation

The below SHapley Additive exPlanation (SHAP) values and visualizations represent feature contributions across the entire two-stage SL model variations. They do not consider contributions to predictions in stage 1 or stage 2, only the overall final prediction (which is a product of stage 1 and 2 predictions itself). This code was also mostly generated by ChatGPT, as I did not know how to implement SHAP values using "fastshap" and "shapviz" before this project.

```{r}
# Wrapper function for passing two-stage SL predictions to fastshap::explain()
pred_twostage <- function(object, newdata) {
  out <- predict_twostageSL_custom(object, newdata)$pred # using custom tssl prediction function
  
  if (!is.numeric(out)) {
    stop("predict() returned non-numeric output") # handling non-numeric predictions
  }

  as.numeric(out)
}
```


Note: the "totexp_tssl" and "totslf_tssl" models used for the SHAP values creation must be the models trained in the above sections, not from the later sensitivity analyses in the following section, which overwrite the model definitions.

## Total Expenditures

```{r}
# Set random seed to same as used in the prior modeling results
set.seed(321)

# Create SHAP values
totexp_shap_values <- explain(
  object       = totexp_tssl,
  X            = as.data.frame(Train %>% select(!c("TOTEXP", "TOTSLF"))),
  newdata      = as.data.frame(Test %>% select(!c("TOTEXP", "TOTSLF"))) ,
  pred_wrapper = pred_twostage,
  nsim         = 100,
  adjust       = TRUE
)

```

```{r}
# Create shapviz object from SHAP values
sv_totexp <- shapviz(
  totexp_shap_values,
  X = as.data.frame(Test %>% select(!c("TOTEXP", "TOTSLF"))),
  baseline = attr(totexp_shap_values, "baseline")
)

# Create Beeswarm plot
sv_importance(sv_totexp, kind = "beeswarm")
```

## Total OOP Expenditure

```{r}
# Set random seed to same as used in the prior modeling results
set.seed(321)

# Creating SHAP values
totslf_shap_values <- explain(
  object       = totslf_tssl,
  X            = as.data.frame(Train %>% select(!c("TOTEXP", "TOTSLF"))),
  newdata      = as.data.frame(Test %>% select(!c("TOTEXP", "TOTSLF"))) ,
  pred_wrapper = pred_twostage,
  nsim         = 100,
  adjust       = TRUE
)

```

```{r}
# Creating shapviz object from SHAP values
sv_totslf <- shapviz(
  totslf_shap_values,
  X = as.data.frame(Test %>% select(!c("TOTEXP", "TOTSLF"))),
  baseline = attr(totexp_shap_values, "baseline")
)

# Creating beeswarm plot
sv_importance(sv_totslf, kind = "beeswarm")
```

# Sensitivity Analysis

The below sensitivity analysis tests the effect of removing the chronic comorbidity covariates on predictions for total expenditures and total out-of-pocket expenditures in a given year. The results tables from these model variations have a "_SA" suffix to differentiate their results from the regular modeling above.

## Total Expenditures Prediction

### Validation Set Predictions

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown above for the TOTEXP validation set prediction code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the validation set. Warnings and excess output have been suppressed using YAML parameters and verbose = FALSE in the model definition.

```{r, message = FALSE, warning = FALSE}
# Reset random seed
set.seed(321)

# Train model on training data and predict on validation set 
totexp_tssl <- twostageSL(
  Y = Train$TOTEXP,
  X = Train %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  newX = Validation %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = FALSE
)

```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner)
data.frame(totexp_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
validation_results_df_totexp_SA <- get_tssl_results_df(tssl_object = totexp_tssl, newY = Validation$TOTEXP)
```

```{r}
# Locally save the results table
# write.csv(validation_results_df_totexp_SA, file = "validation_results_df_totexp_SA.csv")
```

### Test Set Predictions

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown above for the TOTEXP validation set prediction code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the testing set. Warnings and excess output have been suppressed using YAML parameters and verbose = FALSE in the model definition.

```{r, message = FALSE, warning = FALSE}
# Re-training model to predict on unseen test set (no native "predict" function)

# Reset random seed
set.seed(321)

# Re-training model to predict on unseen test set (no working native "predict()" function)
totexp_tssl <- twostageSL(
  Y = Train$TOTEXP,
  X = Train %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  newX = Test %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = FALSE
)
```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner -> should match the earlier coefficients from predicting on the validation set)
data.frame(totexp_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
final_results_df_totexp_SA <- get_tssl_results_df(tssl_object = totexp_tssl, newY = Test$TOTEXP)
```

```{r}
# Locally save the results table
# write.csv(final_results_df_totexp_SA, file = "final_results_df_totexp_SA.csv")
```

## Total OOP Expenditures Prediction

### Validation Set Predictions

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown above for the TOTEXP validation set prediction code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the validation set. Warnings and excess output have been suppressed using YAML parameters and verbose = FALSE in the model definition.

```{r, message = FALSE, warning = FALSE}
# Reset random seed
set.seed(321)

# Train model on training data and predict on validation set 
totslf_tssl <- twostageSL(
  Y = Train$TOTSLF,
  X = Train %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  newX = Validation %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = FALSE,
)
```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner)
data.frame(totslf_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
validation_results_df_totslf_SA <- get_tssl_results_df(tssl_object = totslf_tssl, newY = Validation$TOTSLF)
```

```{r}
# Locally save the results table
# write.csv(validation_results_df_totslf_SA, file = "validation_results_df_totslf_SA.csv")
```

### Test Set Predictions

**Critical note**: the below code will produce "errors", however this does NOT mean the model does not run. As shown above for the TOTEXP validation set prediction code block, the errors only indicate that some models do not converge or experience some other issue that results in their removal from the respective stage of the super learner. Since multiple combinations of algorithms are being considered, some will be removed without stopping the entire sets of algorithms from being considered. The code will still produce a fit model with internal predictions on the testing set. Warnings and excess output have been suppressed using YAML parameters and verbose = FALSE in the model definition.

```{r, message = FALSE, warning = FALSE}
# Reset random seed
set.seed(321)

# Re-training model to predict on unseen test set (no working native "predict()" function) 
totslf_tssl <- twostageSL(
  Y = Train$TOTSLF,
  X = Train %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  newX = Test %>% select(!c("TOTEXP", "TOTSLF", "DIABDX", "HIBPDX", "ANGIDX", "ARTHDX", "ASTHDX", "CABLADDR", "CACERVIX", "CACOLON", "CALUNG", "CALYMPH", "CAMELANO", "CASKINDK", "CASKINNM", "CAUTERUS", "CHDDX", "CHOLDX", "EMPHDX", "MIDX", "OHRTDX")),
  library.2stage = list(stage1 = c("SL.glm", "SL.glmnet", "SL.randomForest", "SL.xgboost"),
                        stage2 = c("SL.lm", "SL.glmnet", "SL.randomForest", "SL.xgboost")),
  library.1stage = c("SL.lm", "SL.glmnet","SL.randomForest", "SL.xgboost"),
  twostage = TRUE,
  family.1 = binomial(),
  family.2 = gaussian(),
  family.single = gaussian(),
  cvControl = list(V = 10),
  verbose = FALSE
)
```

```{r}
# Evaluate which algorithms were selected in the model's two stages (non-zero coefficient = model weight in the two-stage super learner -> should match the earlier coefficients from predicting on the validation set)
data.frame(totslf_tssl$coef)
```

```{r}
# Use custom function to get comparative results data frame
final_results_df_totslf_SA <- get_tssl_results_df(tssl_object = totslf_tssl, newY = Test$TOTSLF)
```

```{r}
# Locally save the results table
# write.csv(final_results_df_totslf_SA, file = "final_results_df_totslf_SA.csv")
```

## View Final Sensitivity Analysis Results Tables

```{r}
final_results_df_totexp_SA %>% View()
```

```{r}
final_results_df_totslf_SA %>% View()
```


# References

\[1\] Z. Wu, S. A. Berkowitz, P. J. Heagerty, and D. Benkeser, A two-stage super learner for healthcare expenditures, Health Services and Outcomes Research Methodology, vol. 22, no. 4, pp. 435453, Jun. 2022. doi:10.1007/s10742-022-00275-x
